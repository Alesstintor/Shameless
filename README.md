# ğŸ­ Shameless - User Sentiment Profiler

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Hacktoberfest 2025](https://img.shields.io/badge/Hacktoberfest-2025-orange.svg)](https://hacktoberfest.com/)
[![Kaggle Model](https://img.shields.io/badge/Kaggle-Model-20BEFF.svg)](https://www.kaggle.com/models/aleselmaquinas/shameless-sentiment-analyzer)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.0-009688.svg)](https://fastapi.tiangolo.com/)
[![Bluesky](https://img.shields.io/badge/Bluesky-Supported-1185FE.svg)](https://bsky.app/)

**Â¿QuÃ© tipo de persona es este usuario en redes sociales?** ğŸ¤”

Shameless analiza el perfil completo de un usuario en **Bluesky** (y Twitter/X) y genera un reporte detallado de su sentimiento. Proporciona un handle de usuario y obtÃ©n anÃ¡lisis completo con:

- âœ¨ Sentimiento general (positivo/negativo)
- ğŸ“Š EstadÃ­sticas detalladas (% positivos, confianza promedio)
- ğŸŒŸ Post mÃ¡s positivo y mÃ¡s negativo
- ğŸ¤– Modelo BERT entrenado en Kaggle (87% accuracy)
- âš¡ API REST lista para producciÃ³n

> ğŸ‰ A project for **Hacktoberfest 2025 A CoruÃ±a**  
> ğŸ¤– Pre-trained model available on [Kaggle Models](https://www.kaggle.com/models/aleselmaquinas/shameless-sentiment-analyzer)

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Documentation](#-documentation)
- [Contributing](#-contributing)
- [Roadmap](#-roadmap)
- [License](#-license)

---

## âœ¨ Features

### ğŸ¯ Core Functionality

- **ğŸ¦‹ Bluesky Support**: Analiza usuarios de Bluesky con autenticaciÃ³n nativa
- **User Analysis**: Proporciona handle â†’ ObtÃ©n perfil de sentimiento completo
- **Smart Scraping**: Recolecta automÃ¡ticamente posts (usando atproto SDK oficial)
- **ğŸ¤– Pre-trained Model**: Modelo DistilBERT listo para usar desde Kaggle (~270 MB)
- **ML-Powered**: Inferencia rÃ¡pida con transformers + PyTorch
- **REST API**: FastAPI con documentaciÃ³n interactiva (Swagger UI)

### ğŸ“Š Analysis Capabilities

- **Overall Sentiment**: Â¿Es el usuario positivo, negativo o neutral?
- **Temporal Analysis**: EvoluciÃ³n del sentimiento a lo largo del tiempo
- **Topic Analysis**: Â¿Sobre quÃ© temas es mÃ¡s positivo/negativo?
- **Engagement Correlation**: RelaciÃ³n entre sentimiento y engagement
- **Insights**: Descubre patrones y tendencias automÃ¡ticamente

### ğŸ¤– Machine Learning

- **ğŸ¯ Pre-trained Model Ready**: DistilBERT fine-tuned on 25k samples (87% accuracy)
- **ğŸ“¦ One-Command Download**: Disponible en [Kaggle Models](https://www.kaggle.com/models/aleselmaquinas/shameless-sentiment-analyzer)
- **âš¡ Kaggle Training**: GPU-accelerated training with T4/P100 (100% gratuito)
- **ğŸ“ Data-Agnostic**: Funciona con tweets, reviews, posts, cualquier texto
- **ğŸ”„ Version Management**: GestiÃ³n de versiones mediante variable de entorno `MODEL_VERSION`
- **ğŸš€ Fast Inference**: ~100ms por texto en CPU, batch processing optimizado
- **ğŸ’¾ Compact Size**: ~270 MB (DistilBERT base + fine-tuning)

### ğŸ¨ User Experience

- **CLI Interface**: Interfaz de lÃ­nea de comandos intuitiva
- **Web UI** (Coming soon): Interface web con Streamlit
- **PDF Reports**: Reportes profesionales exportables
- **Interactive Dashboards**: Visualizaciones interactivas
- **Cache System**: Sistema de cachÃ© para anÃ¡lisis rÃ¡pidos

---

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Collection Layer                                                                                                                        â”‚
â”‚  Bluesky (atproto) â†’ Posts â†’ Storage (JSON)                                                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Preprocessing Layer                                                                                                                            â”‚
â”‚   Text Cleaning â†’ Normalization â†’ Tokenization                                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Machine Learning Layer                                                                                                                       â”‚
â”‚  DistilBERT (Kaggle) â†’ Sentiment Classification â†’ Results                                                                             â”‚
â”‚  Model: v1.0 (~270 MB) | Accuracy: 87% | Inference: ~100ms                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Layer (FastAPI)                                                                                                                         â”‚
â”‚  REST Endpoints â†’ JSON Responses â†’ Interactive Docs                                                                                 â”‚
â”‚  /api/analyze/bluesky/user/{handle}                                                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tech Stack:**

- **Language:** Python 3.11+
- **API Framework:** FastAPI + Uvicorn
- **ML/NLP:** transformers, torch, DistilBERT
- **Social APIs:** atproto (Bluesky), tweepy (Twitter)
- **Data:** pandas, numpy
- **Config:** pydantic-settings, python-dotenv
- **Training:** Kaggle Notebooks (GPU: T4/P100)
- **Model Storage:** Kaggle Models

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or higher
- pip
- Kaggle account (free)

### âš¡ 5-Minute Setup

#### 1. Clone and Setup Environment

```bash
# Clone the repository
git clone https://github.com/Alesstintor/Shameless.git
cd Shameless

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install kaggle  # For downloading the model
```

#### 2. Download the Pre-trained Model

**Option A: Download from Kaggle Web (Recommended for first time)**

1. Go to: https://www.kaggle.com/models/aleselmaquinas/shameless-sentiment-analyzer
2. Click **"Download"** button
3. Extract the downloaded files
4. Copy the extracted `v1.0` folder to: `Sentiment_Analyser/data/models/v1.0`

**Option B: Using Kaggle CLI**

First, configure Kaggle CLI (one-time setup):

1. Go to https://www.kaggle.com/settings/account
2. Click **"Create New Token"** under API section
3. Download `kaggle.json`
4. Place it in:
   - **Windows**: `C:\Users\YOUR_USERNAME\.kaggle\kaggle.json`
   - **Linux/Mac**: `~/.kaggle/kaggle.json` (then run `chmod 600 ~/.kaggle/kaggle.json`)

Then download the model:

```bash
# Navigate to models directory
cd Sentiment_Analyser/data/models

# Download from Kaggle Models
# Visit the model page to find the exact instance version
# https://www.kaggle.com/models/aleselmaquinas/shameless-sentiment-analyzer
# Then use: kaggle models instances versions download <handle>/<model>/<framework>/<instance>/<version>

# Or simply download manually from the web and extract here
# The structure should be: models/v1.0/model/ and models/v1.0/tokenizer/

cd ../../..
```

**Verify the installation:**

```bash
# Check structure (replace v1.0 with your MODEL_VERSION)
ls Sentiment_Analyser/data/models/v1.0/
# Should show: model/ tokenizer/ config.json metrics.json
```

**âš ï¸ IMPORTANTE: Configurar permisos del modelo**

```bash
# Otorgar permisos de lectura al modelo
chmod -R 755 Sentiment_Analyser/data/models/v1.0/
```

> **Note:** The folder name should match your `MODEL_VERSION` setting in `.env`

#### 3. Configure Bluesky Credentials (Optional)

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add your Bluesky credentials:
# BLUESKY_HANDLE=your-handle.bsky.social
# BLUESKY_PASSWORD=your-app-password
```

> **Get Bluesky App Password:** Settings â†’ Privacy and Security â†’ App Passwords â†’ Create New

#### 4. Start the API

```bash
# Start the FastAPI server
uvicorn Sentiment_Analyser.api.main:app --reload
```

Visit: **http://localhost:8000/docs** for interactive API documentation

#### 5. Test It!

```bash
# Analyze sentiment for a Bluesky user
curl "http://localhost:8000/api/analyze/bluesky/user/jay.bsky.team?limit=10"
```

### ğŸ Python Usage

```python
from Sentiment_Analyser.models import SentimentAnalyzer

# Load Kaggle-trained model (version configured in .env)
analyzer = SentimentAnalyzer(
    use_kaggle_model=True,
    kaggle_model_version="v1.0"  # Or set MODEL_VERSION in .env
)

# Analyze single text
result = analyzer.analyze("I love this product!")
print(result)  # {'sentiment': 'positive', 'confidence': 0.9987, 'label': 'LABEL_1'}

# Analyze multiple texts (batch processing)
texts = ["Great experience!", "Terrible service", "Not bad"]
results = analyzer.analyze_batch(texts)
for text, result in zip(texts, results):
    print(f"{text}: {result['sentiment']} ({result['confidence']:.2%})")
```

> **Note:** To use Kaggle models, first train them in Kaggle and download locally.
> See [KAGGLE_WORKFLOW.md](KAGGLE_WORKFLOW.md) for complete instructions.

### ğŸ¯ API Endpoints

**Analyze Bluesky User:**

```bash
GET /api/analyze/bluesky/user/{handle}?limit=10
```

**Response:**

```json
{
  "user_name": "Jay Graber",
  "user_handle": "jay.bsky.team",
  "user_avatar": "https://cdn.bsky.app/...",
  "total_analyzed": 25,
  "positive_count": 18,
  "negative_count": 7,
  "average_confidence": 0.8945,
  "most_positive": { "text": "...", "confidence": 0.9987 },
  "most_negative": { "text": "...", "confidence": 0.8654 },
  "posts": [...]
}
```

---

## ğŸ“¦ Installation

### Complete Setup (Production)

```bash
# 1. Clone and create environment
git clone https://github.com/Alesstintor/Shameless.git
cd Shameless
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt
pip install kaggle

# 3. Download pre-trained model (~270 MB)
# Go to: https://www.kaggle.com/models/aleselmaquinas/shameless-sentiment-analyzer
# Click "Download" and extract to: Sentiment_Analyser/data/models/v1.0
# Note: Folder name must match MODEL_VERSION in .env (default: v1.0)

# 4. Configure environment
cp .env.example .env
# Edit .env with your Bluesky credentials and MODEL_VERSION

# 5. Start API
uvicorn Sentiment_Analyser.api.main:app --reload
```

### Development Installation

```bash
# Install with dev dependencies
pip install -r requirements-dev.txt

# Setup pre-commit hooks
pre-commit install

# Run tests
pytest tests/ -v --cov
```

### Configuration

Edit `.env` with your settings:

```env
# Bluesky Credentials (Required for API)
BLUESKY_HANDLE=your-handle.bsky.social
BLUESKY_PASSWORD=your-app-password  # From Settings â†’ App Passwords

# Model Configuration
MODEL_NAME=distilbert-base-uncased-finetuned-sst-2-english
MODEL_VERSION=v1.0  # Kaggle model version (change to v1.1, v2.0 when available)
MODEL_DEVICE=cpu  # or 'cuda' if you have GPU
MODEL_BATCH_SIZE=32

# API Settings
API_HOST=0.0.0.0
API_PORT=8000

# Logging
LOG_LEVEL=INFO
```

### Troubleshooting

**Error: "Permission denied" al cargar el modelo**

Si recibes errores de permisos al intentar cargar el modelo, asegÃºrate de configurar los permisos correctamente:

```bash
# Otorgar permisos de lectura/escritura al directorio del modelo
chmod -R 755 Sentiment_Analyser/data/models/v1.0/
```

**Error: "Model not found at data/models/v1.0/model"**

The model files are missing. Download them:

1. Go to: https://www.kaggle.com/models/aleselmaquinas/shameless-sentiment-analyzer
2. Click **"Download"** button (you'll need a Kaggle account)
3. Extract the downloaded archive
4. Copy the folder to match your `MODEL_VERSION` in `.env`
   - If `MODEL_VERSION=v1.0`, copy to: `Sentiment_Analyser/data/models/v1.0`
   - If `MODEL_VERSION=v2.0`, copy to: `Sentiment_Analyser/data/models/v2.0`

Expected structure (for v1.0):

```
Sentiment_Analyser/data/models/v1.0/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ model.safetensors
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ vocab.txt
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ special_tokens_map.json
â”œâ”€â”€ config.json
â””â”€â”€ metrics.json
```

> **Tip:** Always ensure the folder name matches the `MODEL_VERSION` variable in your `.env` file

**Error: "Bluesky integration is not configured"**

- Add your credentials to `.env`:
  ```
  BLUESKY_HANDLE=your-handle.bsky.social
  BLUESKY_PASSWORD=your-app-password
  ```
- Get App Password: Bluesky Settings â†’ Privacy and Security â†’ App Passwords

**Slow inference on CPU?**

- Use GPU if available: Set `MODEL_DEVICE=cuda` in `.env`
- Install CUDA-enabled PyTorch:
  ```bash
  pip install torch --index-url https://download.pytorch.org/whl/cu118
  ```

**How to upgrade to a new model version?**

When a new model version is released (e.g., v2.0):

1. Download the new version from Kaggle Models
2. Extract to: `Sentiment_Analyser/data/models/v2.0`
3. Update your `.env`:
   ```env
   MODEL_VERSION=v2.0
   ```
4. Restart the API:
   ```bash
   uvicorn Sentiment_Analyser.api.main:app --reload
   ```

You can keep multiple versions and switch between them by changing `MODEL_VERSION`!

---

## ğŸ’» Usage

### Command Line Interface

```bash
# Collect tweets
python -m sentiment_analyser.scraper collect \
    --query "artificial intelligence" \
    --limit 1000 \
    --output data/raw/tweets.json

# Analyze sentiment
python -m sentiment_analyser.models analyze \
    --input data/raw/tweets.json \
    --output data/processed/analysis.csv
```

### Python API

```python
from sentiment_analyser.scraper import TwitterCollector, DataStorage
from sentiment_analyser.models import SentimentAnalyzer, TextPreprocessor
from sentiment_analyser.config import get_settings

# Initialize components
settings = get_settings()
collector = TwitterCollector(rate_limit=1.0)
preprocessor = TextPreprocessor()
analyzer = SentimentAnalyzer()
storage = DataStorage(settings.RAW_DATA_DIR)

# Collect data
tweets = []
for tweet in collector.search("climate change", limit=500):
    tweets.append(tweet.to_dict())

# Save raw data
storage.save_json(tweets, "climate_tweets.json")

# Preprocess
texts = [tweet['content'] for tweet in tweets]
clean_texts = preprocessor.clean_batch(texts)

# Analyze sentiment
results = analyzer.analyze_batch(clean_texts)

# Process results
for tweet, result in zip(tweets, results):
    print(f"Tweet: {tweet['content'][:50]}...")
    print(f"Sentiment: {result['sentiment']} (confidence: {result['score']:.2%})")
    print("-" * 80)
```

### Jupyter Notebook

Open the example notebook:

```bash
jupyter notebook Sentiment_Analyser/notebooks/sentiment_analysis.ipynb
```

The notebook includes:

- âœ… Complete data collection workflow
- âœ… Preprocessing examples
- âœ… Sentiment analysis
- âœ… Beautiful visualizations
- âœ… Statistical insights
- âœ… Export functionality

---

## ğŸ“ Project Structure

```
Shameless/
â”œâ”€â”€ .windsurf/                    # Windsurf IDE metadata
â”‚   â”œâ”€â”€ architecture.md          # Architecture documentation
â”‚   â”œâ”€â”€ context.md               # Project context and guidelines
â”‚   â””â”€â”€ rules.md                 # Development rules
â”‚
â”œâ”€â”€ Sentiment_Analyser/          # Main package
â”‚   â”œâ”€â”€ config/                  # Configuration management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py          # Settings with pydantic
â”‚   â”‚
â”‚   â”œâ”€â”€ scraper/                 # Data collection
â”‚   â”‚   â”œâ”€â”€ collectors/          # Platform-specific scrapers
â”‚   â”‚   â”‚   â””â”€â”€ twitter_collector.py
â”‚   â”‚   â””â”€â”€ storage/             # Data storage utilities
â”‚   â”‚       â””â”€â”€ data_storage.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # Machine Learning
â”‚   â”‚   â”œâ”€â”€ preprocessing/       # Text preprocessing
â”‚   â”‚   â”‚   â””â”€â”€ text_preprocessor.py
â”‚   â”‚   â””â”€â”€ inference/           # Sentiment analysis
â”‚   â”‚       â””â”€â”€ sentiment_analyzer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                   # Shared utilities
â”‚   â”‚   â””â”€â”€ logger.py            # Logging configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                    # Data storage
â”‚   â”‚   â”œâ”€â”€ raw/                 # Raw scraped data
â”‚   â”‚   â”œâ”€â”€ processed/           # Processed datasets
â”‚   â”‚   â””â”€â”€ models/              # Saved ML models
â”‚   â”‚
â”‚   â””â”€â”€ notebooks/               # Jupyter notebooks
â”‚       â””â”€â”€ sentiment_analysis.ipynb
â”‚
â”œâ”€â”€ tests/                       # Test suite
â”œâ”€â”€ requirements.txt             # Production dependencies
â”œâ”€â”€ requirements-dev.txt         # Development dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ LICENSE                      # MIT License
â””â”€â”€ README.md                    # This file
```

---

## ğŸ“š Documentation

Comprehensive documentation is available in the `.windsurf/` directory:

- **[Architecture Guide](/.windsurf/rules/architecture.md)** - System architecture and design patterns
- **[Development Rules](/.windsurf/rules/rules.md)** - Coding standards and best practices
- **[Project Context](/.windsurf/rules/context.md)** - Project goals, use cases, and workflows

### Additional Resources

- [API Documentation](docs/api.md) _(coming soon)_
- [Deployment Guide](docs/deployment.md) _(coming soon)_
- [Contributing Guide](CONTRIBUTING.md) _(coming soon)_

---

## ğŸ¤ Contributing

We welcome contributions! This project is part of **Hacktoberfest 2025**.

### How to Contribute

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Guidelines

- Follow PEP 8 style guide
- Add type hints to all functions
- Write docstrings (Google style)
- Include unit tests for new features
- Update documentation as needed

### Good First Issues

- ğŸŸ¢ Add support for Reddit scraping
- ğŸŸ¢ Implement additional preprocessing options
- ğŸŸ¢ Create more visualization functions
- ğŸŸ¢ Add multilingual sentiment analysis
- ğŸŸ¢ Improve error handling

---

## ğŸ—ºï¸ Roadmap

### Phase 1: MVP âœ… (Current)

- [x] Project structure and architecture
- [x] Twitter scraper with snscrape
- [x] Basic sentiment analysis (transformers)
- [x] Jupyter notebook for exploration
- [x] Documentation

### Phase 2: Core Features ğŸ”„

- [ ] Advanced ML models (fine-tuned BERT)
- [ ] REST API with FastAPI
- [ ] Database integration (PostgreSQL)
- [ ] Docker containerization
- [ ] CI/CD pipeline

### Phase 3: Scale & Polish ğŸ“…

- [ ] Real-time processing pipeline
- [ ] Web dashboard (React)
- [ ] Model monitoring and retraining
- [ ] Multi-language support
- [ ] Cloud deployment

### Phase 4: Advanced Features ğŸš€

- [ ] Entity recognition
- [ ] Topic modeling
- [ ] Trend prediction
- [ ] Automated reporting
- [ ] Multi-platform support

---

## ğŸ“Š Performance

Current benchmarks (on CPU):

| Metric           | Value                  |
| ---------------- | ---------------------- |
| Scraping Speed   | ~60 tweets/min         |
| Inference Time   | ~100ms/sample          |
| Batch Processing | ~1000 samples/min      |
| Accuracy         | ~87% (SST-2 benchmark) |

---

## ğŸ”’ Security

- âœ… No hardcoded API keys or secrets
- âœ… Environment variables for configuration
- âœ… Input sanitization
- âœ… Rate limiting on scrapers
- âœ… Regular dependency updates

**Report security issues:** Open a private security advisory on GitHub.

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Hacktoberfest 2025 A CoruÃ±a** for the opportunity
- **HuggingFace** for amazing transformer models
- **snscrape** for reliable social media scraping
- All contributors who make this project better

---

<div align="center">

**Made with â¤ï¸ for Hacktoberfest 2025**

â­ **Star this repo** if you find it useful!

</div>
