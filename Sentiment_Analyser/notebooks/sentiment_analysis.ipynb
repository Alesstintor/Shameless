{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Machine Learning\n",
    "\n",
    "This notebook demonstrates how to use the Shameless Sentiment Analyser for:\n",
    "1. Collecting data from Twitter using snscrape\n",
    "2. Preprocessing and cleaning the data\n",
    "3. Performing sentiment analysis using ML models\n",
    "4. Visualizing and analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import our modules\n",
    "from sentiment_analyser.scraper import TwitterCollector, DataStorage\n",
    "from sentiment_analyser.models import SentimentAnalyzer, TextPreprocessor\n",
    "from sentiment_analyser.config import get_settings\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get settings\n",
    "settings = get_settings()\n",
    "\n",
    "# Configuration parameters\n",
    "SEARCH_QUERY = \"python programming\"  # Change this to your search term\n",
    "MAX_TWEETS = 100  # Number of tweets to collect\n",
    "SINCE_DATE = (datetime.now() - timedelta(days=7)).strftime(\"%Y-%m-%d\")  # Last 7 days\n",
    "\n",
    "print(f\"Search Query: {SEARCH_QUERY}\")\n",
    "print(f\"Max Tweets: {MAX_TWEETS}\")\n",
    "print(f\"Since Date: {SINCE_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection\n",
    "\n",
    "Collect tweets using snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collector = TwitterCollector(rate_limit=1.0)\n",
    "\n",
    "# Collect tweets\n",
    "print(f\"Collecting tweets for query: '{SEARCH_QUERY}'...\")\n",
    "tweets = []\n",
    "\n",
    "for tweet in collector.search(\n",
    "    query=SEARCH_QUERY,\n",
    "    limit=MAX_TWEETS,\n",
    "    since=SINCE_DATE\n",
    "):\n",
    "    tweets.append(tweet.to_dict())\n",
    "    \n",
    "    # Progress update\n",
    "    if len(tweets) % 20 == 0:\n",
    "        print(f\"Collected {len(tweets)} tweets...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Collected {len(tweets)} tweets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of collected tweets:\")\n",
    "df[['user', 'content', 'likes', 'retweets', 'date']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total tweets: {len(df)}\")\n",
    "print(f\"Unique users: {df['username'].nunique()}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nEngagement stats:\")\n",
    "print(df[['likes', 'retweets', 'replies']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tweet distribution over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Tweets over time\n",
    "df.set_index('date').resample('D').size().plot(ax=axes[0, 0], kind='bar')\n",
    "axes[0, 0].set_title('Tweets per Day')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Number of Tweets')\n",
    "\n",
    "# Engagement distribution\n",
    "axes[0, 1].hist(df['likes'], bins=30, alpha=0.7, label='Likes')\n",
    "axes[0, 1].hist(df['retweets'], bins=30, alpha=0.7, label='Retweets')\n",
    "axes[0, 1].set_title('Engagement Distribution')\n",
    "axes[0, 1].set_xlabel('Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Top users\n",
    "top_users = df['username'].value_counts().head(10)\n",
    "top_users.plot(ax=axes[1, 0], kind='barh')\n",
    "axes[1, 0].set_title('Top 10 Most Active Users')\n",
    "axes[1, 0].set_xlabel('Number of Tweets')\n",
    "\n",
    "# Language distribution\n",
    "df['language'].value_counts().head(10).plot(ax=axes[1, 1], kind='pie', autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Language Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_urls=True,\n",
    "    remove_mentions=False,\n",
    "    remove_hashtags=False,\n",
    "    remove_emojis=False\n",
    ")\n",
    "\n",
    "# Clean texts\n",
    "df['clean_content'] = preprocessor.clean_batch(df['content'].tolist())\n",
    "\n",
    "# Compare original vs cleaned\n",
    "print(\"Original vs Cleaned Text Examples:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"Original: {df.iloc[i]['content'][:100]}...\")\n",
    "    print(f\"Cleaned:  {df.iloc[i]['clean_content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentAnalyzer(\n",
    "    model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=\"cpu\",\n",
    "    preprocess=False  # We already preprocessed\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Sentiment analyzer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiments in batch\n",
    "print(\"Analyzing sentiments...\")\n",
    "sentiments = analyzer.analyze_batch(df['clean_content'].tolist(), batch_size=32)\n",
    "\n",
    "# Add results to dataframe\n",
    "df['sentiment'] = [s['sentiment'] for s in sentiments]\n",
    "df['sentiment_score'] = [s['score'] for s in sentiments]\n",
    "df['sentiment_label'] = [s['label'] for s in sentiments]\n",
    "\n",
    "print(\"\\n‚úÖ Sentiment analysis complete!\")\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "print(\"\\nSample Results:\")\n",
    "df[['content', 'sentiment', 'sentiment_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = {'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#95a5a6'}\n",
    "sentiment_colors = [colors.get(s, '#3498db') for s in sentiment_counts.index]\n",
    "\n",
    "axes[0, 0].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "               colors=sentiment_colors, startangle=90)\n",
    "axes[0, 0].set_title('Overall Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Sentiment over time\n",
    "df_time = df.set_index('date')\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    sentiment_data = df_time[df_time['sentiment'] == sentiment]\n",
    "    sentiment_by_day = sentiment_data.resample('D').size()\n",
    "    axes[0, 1].plot(sentiment_by_day.index, sentiment_by_day.values, \n",
    "                    marker='o', label=sentiment.capitalize(), \n",
    "                    color=colors.get(sentiment, '#3498db'))\n",
    "\n",
    "axes[0, 1].set_title('Sentiment Trend Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Number of Tweets')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence score distribution\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    sentiment_data = df[df['sentiment'] == sentiment]['sentiment_score']\n",
    "    axes[1, 0].hist(sentiment_data, bins=20, alpha=0.6, label=sentiment.capitalize(),\n",
    "                    color=colors.get(sentiment, '#3498db'))\n",
    "\n",
    "axes[1, 0].set_title('Sentiment Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Confidence Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sentiment vs Engagement\n",
    "sentiment_engagement = df.groupby('sentiment').agg({\n",
    "    'likes': 'mean',\n",
    "    'retweets': 'mean',\n",
    "    'replies': 'mean'\n",
    "})\n",
    "\n",
    "x = range(len(sentiment_engagement))\n",
    "width = 0.25\n",
    "axes[1, 1].bar([i - width for i in x], sentiment_engagement['likes'], \n",
    "               width, label='Likes', color='#3498db')\n",
    "axes[1, 1].bar(x, sentiment_engagement['retweets'], \n",
    "               width, label='Retweets', color='#2ecc71')\n",
    "axes[1, 1].bar([i + width for i in x], sentiment_engagement['replies'], \n",
    "               width, label='Replies', color='#e74c3c')\n",
    "\n",
    "axes[1, 1].set_title('Average Engagement by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(sentiment_engagement.index)\n",
    "axes[1, 1].set_ylabel('Average Count')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Insights and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most positive tweets\n",
    "print(\"üåü Most Positive Tweets:\\n\")\n",
    "positive_tweets = df[df['sentiment'] == 'positive'].nlargest(5, 'sentiment_score')\n",
    "for idx, row in positive_tweets.iterrows():\n",
    "    print(f\"Score: {row['sentiment_score']:.4f}\")\n",
    "    print(f\"Tweet: {row['content'][:150]}...\")\n",
    "    print(f\"By: @{row['username']} | Likes: {row['likes']} | Retweets: {row['retweets']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most negative tweets\n",
    "print(\"‚ö†Ô∏è  Most Negative Tweets:\\n\")\n",
    "negative_tweets = df[df['sentiment'] == 'negative'].nlargest(5, 'sentiment_score')\n",
    "for idx, row in negative_tweets.iterrows():\n",
    "    print(f\"Score: {row['sentiment_score']:.4f}\")\n",
    "    print(f\"Tweet: {row['content'][:150]}...\")\n",
    "    print(f\"By: @{row['username']} | Likes: {row['likes']} | Retweets: {row['retweets']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"üìä Summary Statistics:\\n\")\n",
    "print(f\"Average sentiment score: {df['sentiment_score'].mean():.4f}\")\n",
    "print(f\"Positive tweets: {(df['sentiment'] == 'positive').sum()} ({(df['sentiment'] == 'positive').sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Negative tweets: {(df['sentiment'] == 'negative').sum()} ({(df['sentiment'] == 'negative').sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Neutral tweets: {(df['sentiment'] == 'neutral').sum()} ({(df['sentiment'] == 'neutral').sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí¨ Engagement by Sentiment:\")\n",
    "print(df.groupby('sentiment')[['likes', 'retweets', 'replies']].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage\n",
    "storage = DataStorage(settings.PROCESSED_DATA_DIR)\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"sentiment_analysis_{timestamp}\"\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = storage.save_csv(df.to_dict('records'), f\"{filename}.csv\")\n",
    "print(f\"‚úÖ Results saved to: {csv_path}\")\n",
    "\n",
    "# Save as Parquet (more efficient)\n",
    "parquet_path = storage.save_parquet(df.to_dict('records'), f\"{filename}.parquet\")\n",
    "print(f\"‚úÖ Results saved to: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ‚úÖ Collecting tweets using snscrape\n",
    "- ‚úÖ Preprocessing and cleaning text data\n",
    "- ‚úÖ Performing sentiment analysis with ML models\n",
    "- ‚úÖ Visualizing and analyzing results\n",
    "- ‚úÖ Saving processed data\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different search queries and hashtags\n",
    "2. Experiment with different ML models\n",
    "3. Add more advanced visualizations\n",
    "4. Implement real-time monitoring\n",
    "5. Build a dashboard for results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
