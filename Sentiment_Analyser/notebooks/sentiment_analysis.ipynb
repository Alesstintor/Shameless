{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model Training\n",
    "\n",
    "**Objetivo:** Entrenar un modelo de analisis de sentimientos usando Sentiment140 dataset (1.6M tweets)\n",
    "\n",
    "**Workflow:**\n",
    "1. Cargar dataset de Kaggle (Sentiment140)\n",
    "2. Preprocesar y dividir datos\n",
    "3. Entrenar modelo DistilBERT\n",
    "4. Evaluar performance\n",
    "5. Guardar modelo para uso local\n",
    "\n",
    "**Modelo resultante:**\n",
    "- Acepta 1 o multiples textos\n",
    "- Funciona con tweets, reviews, cualquier texto corto\n",
    "- Listo para inferencia rapida en produccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Internet Connection\n",
    "import urllib.request\n",
    "\n",
    "def check_internet():\n",
    "    try:\n",
    "        urllib.request.urlopen('https://huggingface.co', timeout=3)\n",
    "        print(\"Internet connection: OK\")\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Internet connection: FAILED\")\n",
    "        print(\"\\nIMPORTANT: Enable Internet in Kaggle!\")\n",
    "        print(\"1. Settings (top right) -> Internet -> ON\")\n",
    "        print(\"2. Save & Run All\")\n",
    "        return False\n",
    "\n",
    "has_internet = check_internet()\n",
    "\n",
    "if not has_internet:\n",
    "    print(\"\\nCannot proceed without internet to download models.\")\n",
    "    print(\"Please enable internet and restart the notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tokenizer files manually to avoid template search bug\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"Downloading tokenizer files manually...\")\n",
    "\n",
    "# Create local directory\n",
    "tokenizer_dir = \"/kaggle/working/distilbert_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "\n",
    "# Files to download from HuggingFace\n",
    "base_url = \"https://huggingface.co/distilbert-base-uncased/resolve/main\"\n",
    "files = {\n",
    "    \"tokenizer_config.json\": f\"{base_url}/tokenizer_config.json\",\n",
    "    \"vocab.txt\": f\"{base_url}/vocab.txt\", \n",
    "    \"tokenizer.json\": f\"{base_url}/tokenizer.json\",\n",
    "    \"special_tokens_map.json\": f\"{base_url}/special_tokens_map.json\",\n",
    "    \"config.json\": f\"{base_url}/config.json\"\n",
    "}\n",
    "\n",
    "for filename, url in files.items():\n",
    "    filepath = os.path.join(tokenizer_dir, filename)\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"[OK] {filename}\")\n",
    "        else:\n",
    "            print(f\"[ERROR] {filename} - Status {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {filename} - Error: {e}\")\n",
    "\n",
    "print(f\"\\nTokenizer files ready at: {tokenizer_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early import of torch to check GPU\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch check:\")\n",
    "print(f\"  Version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"  WARNING: No GPU detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ML & NLP\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,  # Use specific tokenizer to avoid template search\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MODEL_VERSION = \"v1.0\"\n",
    "MAX_LENGTH = 128  # Token length for tweets/short texts\n",
    "BATCH_SIZE = 64  # Increased from 32 to 64 for faster training\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data Configuration  \n",
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Sample size (set to None to use full dataset)\n",
    "SAMPLE_SIZE = 100000  # Use 100k for faster training, None for full 1.6M\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = \"./shameless_sentiment_model\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE} (optimized for speed)\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Sample Size: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full dataset'}\")\n",
    "print(f\"FP16: {torch.cuda.is_available()}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "**Dataset:** Sentiment140 (1.6M tweets)\n",
    "- URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "- Add this dataset to your notebook in Kaggle before running\n",
    "\n",
    "Este dataset contiene 1.6 millones de tweets etiquetados automaticamente:\n",
    "- 0 = Negative sentiment\n",
    "- 4 = Positive sentiment (convertiremos a 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentiment140 dataset from Kaggle\n",
    "# Dataset: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "print(\"Loading Sentiment140 dataset...\")\n",
    "\n",
    "# First, check what files are available\n",
    "print(\"\\nAvailable input files:\")\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Try to find the correct path\n",
    "possible_paths = [\n",
    "    '/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv',\n",
    "    '/kaggle/input/sentiment140/testdata.manual.2009.06.14.csv',\n",
    "    '/kaggle/input/sentiment140/sentiment140.csv',\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        csv_path = path\n",
    "        print(f\"\\nFound dataset at: {csv_path}\")\n",
    "        break\n",
    "\n",
    "# If not found in expected paths, use the first CSV found\n",
    "if csv_path is None:\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                csv_path = os.path.join(dirname, filename)\n",
    "                print(f\"\\nUsing dataset: {csv_path}\")\n",
    "                break\n",
    "        if csv_path:\n",
    "            break\n",
    "\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\"No CSV file found in /kaggle/input/. Please add the Sentiment140 dataset.\")\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path, \n",
    "                 encoding='latin-1', \n",
    "                 header=None,\n",
    "                 names=['sentiment', 'id', 'date', 'query', 'user', 'text'])\n",
    "\n",
    "# Convert sentiment: 0 (negative) stays 0, 4 (positive) becomes 1\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "# Keep only text and sentiment\n",
    "df = df[['text', 'sentiment']]\n",
    "\n",
    "# Sample for faster training (remove for full dataset)\n",
    "if SAMPLE_SIZE:\n",
    "    df = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42).reset_index(drop=True)\n",
    "    print(f\"\\nUsing sample of {len(df):,} texts\")\n",
    "else:\n",
    "    print(f\"\\nUsing full dataset: {len(df):,} texts\")\n",
    "\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nSample texts:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "Crear un dataset PyTorch **agnostico** que funcione con cualquier texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset agnostico para analisis de sentimientos.\n",
    "    Funciona con cualquier texto de entrada (tweets, reviews, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"Dataset class created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "print(\"Splitting dataset...\")\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['sentiment'].values,\n",
    "    test_size=(VAL_SIZE + TEST_SIZE),\n",
    "    random_state=42,\n",
    "    stratify=df['sentiment'].values\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts,\n",
    "    temp_labels,\n",
    "    test_size=TEST_SIZE/(VAL_SIZE + TEST_SIZE),\n",
    "    random_state=42,\n",
    "    stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer from local directory (avoids template search)\n",
    "print(f\"Loading tokenizer from local files...\")\n",
    "\n",
    "tokenizer_dir = \"/kaggle/working/distilbert_tokenizer\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    tokenizer_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"Tokenizer loaded successfully\")\n",
    "\n",
    "# Create datasets with torch format for optimization\n",
    "print(\"\\nCreating optimized datasets...\")\n",
    "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"Datasets created: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "print(f\"Expected steps per epoch: {len(train_dataset) // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,  # Binary classification: positive/negative\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "print(f\"\\nMoving model to {DEVICE}...\")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Verify model is on correct device\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    \n",
    "print(f\"\\nModel loaded successfully on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments (OPTIMIZED for Kaggle GPU)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Evaluation strategy\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    \n",
    "    # Early stopping (optional)\n",
    "    # Uncomment to stop if no improvement for 1 epoch:\n",
    "    # early_stopping_patience=1,\n",
    "    \n",
    "    # Logging - less frequent for cleaner output\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,  # Changed from 10 to 50 for less noise\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_total_limit=2,  # Keep only best 2 checkpoints\n",
    "    \n",
    "    # Performance optimizations\n",
    "    fp16=torch.cuda.is_available(),  # Half precision for speed\n",
    "    dataloader_num_workers=2,  # Parallel data loading\n",
    "    gradient_accumulation_steps=1,  # No accumulation (batch size is good)\n",
    "    \n",
    "    # Disable wandb if not needed\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard logging\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured with optimizations:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE} (2x faster)\")\n",
    "print(f\"  - FP16: Enabled\")\n",
    "print(f\"  - Logging every: 50 steps\")\n",
    "print(f\"  - Data workers: 2\")\n",
    "print(f\"  - Early stopping: Disabled (can enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with progress tracking\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(train_dataset) // BATCH_SIZE}\")\n",
    "print(f\"Total steps: {(len(train_dataset) // BATCH_SIZE) * EPOCHS}\")\n",
    "print(f\"Total epochs: {EPOCHS}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNOTE: In Kaggle, progress bars may not show in real-time.\")\n",
    "print(\"You will see loss/metrics printed every 50 steps.\")\n",
    "print(\"Expected time: ~10-15 minutes with GPU (batch_size=64)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Force output flush\n",
    "import sys\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Start training\n",
    "start_time = datetime.now()\n",
    "print(f\"Training started at: {start_time.strftime('%H:%M:%S')}\\n\")\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    training_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    print(f\"\\nFinal training metrics:\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions for detailed analysis\n",
    "print(\"Getting predictions for detailed analysis...\")\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    target_names=['Negative', 'Positive']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Save Model\n",
    "\n",
    "Guardar el modelo entrenado para crear un Kaggle Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR) / MODEL_VERSION\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to: {output_path}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_path / \"model\")\n",
    "tokenizer.save_pretrained(output_path / \"tokenizer\")\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"model_version\": MODEL_VERSION,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"num_labels\": 2,\n",
    "    \"label_mapping\": {\"0\": \"negative\", \"1\": \"positive\"},\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"training_time_seconds\": training_time,\n",
    "    \"device_used\": DEVICE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"dataset_size\": len(df)\n",
    "}\n",
    "\n",
    "with open(output_path / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Save metrics\n",
    "metrics_data = {\n",
    "    \"test_accuracy\": float(test_results.get(\"eval_accuracy\", 0)),\n",
    "    \"test_f1\": float(test_results.get(\"eval_f1\", 0)),\n",
    "    \"test_loss\": float(test_results.get(\"eval_loss\", 0)),\n",
    "    \"train_loss\": float(train_result.metrics.get(\"train_loss\", 0)),\n",
    "    \"classification_report\": classification_report(\n",
    "        true_labels, pred_labels, \n",
    "        target_names=['Negative', 'Positive'],\n",
    "        output_dict=True\n",
    "    )\n",
    "}\n",
    "\n",
    "with open(output_path / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"\\nModel files:\")\n",
    "print(f\"  - model/\")\n",
    "print(f\"  - tokenizer/\")\n",
    "print(f\"  - config.json\")\n",
    "print(f\"  - metrics.json\")\n",
    "print(f\"\\nTest Accuracy: {metrics_data['test_accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score: {metrics_data['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: Create Kaggle Dataset\n",
    "\n",
    "Para usar este modelo localmente:\n",
    "\n",
    "1. **Guardar como Kaggle Dataset:**\n",
    "   - En este notebook, ve a \"Save Version\"\n",
    "   - El modelo se guarda en `/kaggle/working/shameless_sentiment_model/v1.0/`\n",
    "   - Crea un nuevo Dataset desde este output\n",
    "\n",
    "2. **Descargar localmente:**\n",
    "   ```bash\n",
    "   kaggle datasets download YOUR_USERNAME/shameless-sentiment-model\n",
    "   unzip shameless-sentiment-model.zip -d data/models/v1.0/\n",
    "   ```\n",
    "\n",
    "3. **Usar en tu aplicacion:**\n",
    "   ```python\n",
    "   from sentiment_analyser.models import SentimentAnalyzer\n",
    "   analyzer = SentimentAnalyzer(use_kaggle_model=True, kaggle_model_version=\"v1.0\")\n",
    "   result = analyzer.analyze(\"I love this!\")\n",
    "   ```\n",
    "\n",
    "**Training Complete!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
